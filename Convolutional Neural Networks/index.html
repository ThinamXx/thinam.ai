<p><img src="https://github.com/ThinamXx/thinam.ai/blob/master/images/Neural-Network.png?raw=true" alt="Image" /></p>

<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>

<p>The five architectures of CNNs that have been <em>pre-trained</em> on the ImageNet dataset and, are present in the <em>Keras</em> library are mentioned below:</p>

<ol>
  <li>
    <p>VGG16</p>
  </li>
  <li>
    <p>VGG19</p>
  </li>
  <li>
    <p>ResNet50</p>
  </li>
  <li>
    <p>Inception V3</p>
  </li>
  <li>
    <p>Xception</p>
  </li>
</ol>

<h2 id="parameterized-learning">Parameterized Learning</h2>

<p>The point of parameterized learning is to define a machine learning model that can learn patterns from our input data during training time, but have the testing process be must faster and to obtain a model that can be defined using a small number of parameters that can easily represent the network regardless of training size.</p>

<h2 id="vgg16--vgg19">VGG16 &amp; VGG19</h2>

<p><img src="https://github.com/ThinamXx/thinam.ai/blob/master/images/CNNs/image1.png?raw=true" alt="Image" /></p>

<p>Fig: A visualization of the VGG architecture.</p>

<p>The figure presents the visualization of the VGG architecture. Images with <em>224 x 224 x3</em> dimensions are inputted to the network. Convolutions filters of only <em>3 x 3</em> are then applied with more convolutions stacked on top of each other prior to max pooling operations deeper in the architecture.</p>

<p>The VGG network architecture was introduced by Simonyan and Zisserman in their 2014 paper, <em>Very Deep Convolutional Networks for Large Scale Image Recognition.</em> The VGG family of networks is characterized by using only <em>3 x 3</em> convolutional layers stacked on top of each other in increasing depth. The volume size is reduced by max pooling. Two fully-connected layers are then followed by a softmax classifier.</p>

<h2 id="resnet">ResNet</h2>

<p><img src="https://github.com/ThinamXx/thinam.ai/blob/master/images/CNNs/image2.png?raw=true" alt="Image" /></p>

<p>Fig: <strong>Left:</strong> The original residual module. <strong>Right:</strong> The updated residual module using pre-activation.</p>

<p>The <strong>ResNet</strong> module was introduced by He et al. in their 2015 paper, <em>Deep Residual Learning for Image Recognition.</em> The ResNet architecture has become a seminal work in the deep learning literature, demonstrating that extremely deep networks can be trained using standard SGD through the use of residual modules. Accuracy can be obtained by updating the residual module to use identity mappings.</p>

<h2 id="inception-v3">Inception V3</h2>

<p><img src="https://github.com/ThinamXx/thinam.ai/blob/master/images/CNNs/image3.png?raw=true" alt="Image" /></p>

<p>Fig: The original Inception module used in GoogLeNet.</p>

<p>The Inception module was introduced by Szegedy et al. in their 2014 paper, <em>Going Deeper with Convolutions.</em> The goal of the Inception module is to act as multi-level feature extractor by computing <em>1 x 1, 3 x 3,</em> and <em>5 x 5</em> convolutions within the same module of the network. The output of these filters are then stacked along the channel dimension before being fed into the next layer in the network. The original incarnation of this architecture was called <strong>GoogLeNet.</strong></p>

<h2 id="xception">Xception</h2>

<p>Xception module was introduced by Francois Chollet in their 2016 paper, <em>Xception: Deep Learning with Depthwise Separable Convolutions.</em> Xception is an extension to the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.</p>
