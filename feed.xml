<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://thinamxx.github.io/thinam.ai/feed.xml" rel="self" type="application/atom+xml" /><link href="https://thinamxx.github.io/thinam.ai/" rel="alternate" type="text/html" /><updated>2022-04-03T04:10:33-05:00</updated><id>https://thinamxx.github.io/thinam.ai/feed.xml</id><title type="html">THINAM TAMANG</title><subtitle>Personal website for Thinam Tamang</subtitle><entry><title type="html">Data</title><link href="https://thinamxx.github.io/thinam.ai/Data%20Preparation/" rel="alternate" type="text/html" title="Data" /><published>2022-04-03T00:00:00-05:00</published><updated>2022-04-03T00:00:00-05:00</updated><id>https://thinamxx.github.io/thinam.ai/DataPreparation</id><author><name>Thinam Tamang</name></author><category term="machine learning" /><category term="data preparation" /><category term="sampling" /><category term="data versioning" /><summary type="html">Outliers Outliers are examples that look dissimilar to the majority of examples from the dataset. Dissimilarity is measured by some distance metric, such as Euclidean distance. Deleting outliers from the training dataset is not considered scientifically significant, especially in small datasets. In the big data context, outliers don’t typically have a significant impact on the model. Data Leakage Data leakage, also known as target leakage, is a problem affecting several stages of the machine learning life cycle, from data collection to model evaluation. Data leakage in supervised learning is the unintentional introduction of information about the target that shouldn’t be made available. What is Good Data Good data is informative. Good data has good coverage. Good data reflects real inputs. Good data is unbiased. Good data is not a result of a feedback loop. Good data has consistent labels. Good data is big enough to allow generalization. Data Augmentation The most effective strategy applied to images to get more labeled examples without additional labeling is called data augmentation. The simple operations are flip, rotation, crop, color shift, noise addition, perspective correction, contrast change, and information loss. Mixup is the popular technique of data augmentation which consists of training the model on a mix of the images from the training set. Instead of training the model on the raw images, we take two images and use for training their linear combination: mixup_image = t x image₁ + (1 - t) x image₂ mixup_target = t x target₁ + (1 - t) x target₂ Oversampling Oversampling is a technique to mitigate the class imbalance by making multiple copies of minority class examples. Two popular algorithms that oversample the minority class by creating synthetic examples: Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic Sampling Method (ADASYN). Undersampling Undersampling is a technique to mitigate the class imbalance by removing some examples from the training set of the majority class based on the property called Tomek links. A Tomek link exists between two examples xi and xj belonging to two classes if there’s no other examples xk in the dataset closer to either xi or xj than the latter two are to each other. Data Sampling In probability sampling, all examples have a chance of being selected and it involves randomness. Nonprobability sampling is not random and it follows a fixed deterministic sequence of heuristic actions which means that some examples don’t have a chance of being selected, no matter how many samples we build. In stratified sampling, we first divide our dataset into groups called strata and then randomly select examples from each stratum, like in simple random sampling. It often improves the representativeness of the sample by reducing its bias. Data Versioning If data is held and updated in multiple places, we might need to keep track of versions. Versioning the data is also needed if we frequently update the model by collecting more data, especially in an automated way. Data versioning can be implemented in several levels of complexity. Level 0: data is unversioned. Level 1: data is versioned as a snapshot at training time. Level 2: both data and code are versioned as one asset. Level 3: using or building a specialized data versioning solution. Data Lake A data lake is a repository of data stored in its natural or raw format, usually in the form of object blobs or files. A data lake is typically an unstructured aggregation of data from multiple sources, including databases, logs, or intermediary data obtained as a result of expensive transformations of the original data.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/DataPrep.png" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/DataPrep.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Machine Learning</title><link href="https://thinamxx.github.io/thinam.ai/Machine%20Learning/" rel="alternate" type="text/html" title="Machine Learning" /><published>2022-03-27T00:00:00-05:00</published><updated>2022-03-27T00:00:00-05:00</updated><id>https://thinamxx.github.io/thinam.ai/MachineLearning</id><author><name>Thinam Tamang</name></author><category term="deep learning" /><category term="66DaysOfData" /><category term="machine learning" /><category term="bias" /><summary type="html">Machine Learning Machine learning can be defined as the process of solving a practical problem by collecting a dataset, and algorithmically training a statistical model based on that dataset. Supervised Learning The goal of a supervised learning algorithm is to use a dataset to produce a model that takes a feature vector x as input and outputs information that allows deducing a label for this feature vector. For instance, a model created using a dataset of patients could take as input a feature vector describing a patient and output a probability that the patient has cancer. Unsupervised Learning The goal of an unsupervised learning algorithm is to create a model that takes a feature vector x as input and either transforms it into another vector or into a value that can be used to solve a practice problem. Clustering is useful for finding groups of similar objects in a large collection of objects, such as images or text documents. Reinforcement Learning Reinforcement learning is a subfield of machine learning where the machine also called an agent lives in an environment and is capable of perceiving the state of that environment as a vector of features. A common goal of reinforcement learning is to learn a function that takes the feature vector of a state as input and outputs an optimal action to execute in that state. The action is optimal if it maximizes the expected average long-term reward. Raw and Tidy Data Raw data is a collection of entities in their natural form: they cannot always be directly fed to machine learning algorithms. Tidy data can be seen as a spreadsheet, in which each row represents one example, and columns represent various attributes of an example. Training and Holdout Sets The first step in a machine learning project is to shuffle the examples and split them into three distinct sets: training, validation, and test. The learning algorithm uses the training set to produce the model. The validation set is used to choose the learning algorithm, and find the best configuration values for that learning algorithm, also known as hyperparameters. The test set is used to assess the model performance before delivering it to the client or putting the model into production. Parameters vs. Hyperparameters Hyperparameters are inputs of machine learning algorithms or pipelines that influence the performance of the model. They don’t belong to the training data and cannot be learned from it. Parameters are variables that define the model trained by the learning algorithm. Parameters are directly modified by the learning algorithm based on the training data to find the optimal values of the parameters. When to use Machine Learning When the problem is too complex for coding. When the problem is constantly changing. When it is a perceptive problem such as speech, image, and video recognition. When it is an unstudied phenomenon. When the problem has a simple objective. Machine Learning Engineering Machine learning engineering is the use of scientific principles, tools and techniques of machine learning and traditional software engineering to design and build complex computing systems. MLE ecompasses all stages from data collection, to model training, to making the model available for use by the customers. MLE includes any activity that lets machine learning algorithms be implemented as a part of an effective production system. A machine learning project life cycle consists of the following steps: Goal definition. Data collection and preparation. Feature engineering. Model training. Model evaluation. Model deployment. Model serving. Model monitoring. Model maintenance. Bias Bias in data is an inconsistency with the phenomenon that data represents. Selection bias is the tendency to skew your choice of data sources to those that are easily available, convenient, and cost-effective. Omitted variable bias happens when your featurized data doesn’t have a feature necessary for accurate prediction. Sampling bias also known as distribution shift occurs when the distribution of examples used for training doesn’t reflect the distribution of the inputs the model will receive in production. It is usually impossible to know exactly what biases are present in a dataset. Biases can be avoided by questioning everything: who created the data, what were their motivations and quality criteria, and more importantly, how and why the data was created.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/ML.jpg" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/ML.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Pattern Recognition &amp;amp; ML</title><link href="https://thinamxx.github.io/thinam.ai/Pattern%20Recognition/" rel="alternate" type="text/html" title="Pattern Recognition &amp;amp; ML" /><published>2022-02-10T00:00:00-06:00</published><updated>2022-02-10T00:00:00-06:00</updated><id>https://thinamxx.github.io/thinam.ai/Pattern</id><author><name>Thinam Tamang</name></author><category term="machine learning" /><category term="pattern recognition" /><category term="notes" /><category term="neural networks" /><summary type="html">Introduction The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. Generalization, the ability to categorize correctly new examples that differ from those used for training is a central goal in pattern recognition. Applications in which the training data comprises examples of the input vectors with their corresponding target vectors are known as supervised learning problems. The cases such as the digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called classification. If the desired output consists of one or more continuous variables, then the task is called regression. The pattern recognition problems in which the training data consists of a set of input vectors x without any corresponding target values are called unsupervised learning problems. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization. The technique of reinforcement learning is concerned with the problem of finding suitable actions to take in a given situation in order to maximize a reward. A general feature of reinforcement learning is the trade-off between exploration, in which the system tries out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward. Probability Theory Probability theory provides a consistent framework for the quantification and manipulation of uncertainty. When combined with decision theory, it allows us to make optimal predictions given all the information available to us, even though that information may be incomplete or ambiguous. The Rules of Probability Sum Rule: p(X) = $\sum_{Y}^{}{p(X,\ Y)}$ Product Rule: p(X, Y) = p(Y / X)p(X) Here, p(X, Y) is a joint probability and is verbalized as “the probability of X and Y”. Similarly, the quantity p(Y/X) is a conditional probability and is verbalized as “the probability of Y given X”, where the quantity p(X) is a marginal probability and is verbalized as “the probability of X”. Bayes’ Theorem From the product rule, together with the symmetry property p(X, Y) = p(Y, X), we obtain the following relationship between conditional probabilities which is called Bayes’ theorem which plays a central role in pattern recognition and machine learning. Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator as being the normalization constant required ensuring that the sum of the conditional probability on the left-hand side over all values of Y equals one. p(Y / X) = $\frac{p\left( \frac{X}{Y} \right)p(X)}{p(X)}$ p(X) = $\sum_{Y}^{}{p\left( \frac{X}{Y} \right)p(Y)}$ Model Selection In the maximum likelihood approach, the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. If data is plentiful, the one approach is simply to use some of the available data to train a range of models, or a given model with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the best predictive performance. However, in many applications, the data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. If the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this problem is to use cross-validation. This allows a proportion (S – 1) / S of the available data to be used for training while making use of all of the data to assess performance. Decision Theory If we have an input vector x together with a corresponding vector t of target variables, our goal is to predict t given a new value for x. For regression problems, t will comprise continuous variables, whereas for classification problems, t will represent class labels. Minimizing Loss Function Loss function is also called a cost function, which is a single, overall measure of loss incurred in taking any of the available decisions or actions. Our goal is then to minimize the total loss incurred.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/Pattern.jpg" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/Pattern.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Convolutional Neural Networks Architectures</title><link href="https://thinamxx.github.io/thinam.ai/Convolutional%20Neural%20Networks/" rel="alternate" type="text/html" title="Convolutional Neural Networks Architectures" /><published>2022-01-22T00:00:00-06:00</published><updated>2022-01-22T00:00:00-06:00</updated><id>https://thinamxx.github.io/thinam.ai/CNNs</id><author><name>Thinam Tamang</name></author><category term="deep learning" /><category term="66DaysOfData" /><category term="notes" /><category term="cnns" /><category term="resnet" /><summary type="html">Convolutional Neural Networks The five architectures of CNNs that have been pre-trained on the ImageNet dataset and, are present in the Keras library are mentioned below: VGG16 VGG19 ResNet50 Inception V3 Xception Parameterized Learning The point of parameterized learning is to define a machine learning model that can learn patterns from our input data during training time, but have the testing process be must faster and to obtain a model that can be defined using a small number of parameters that can easily represent the network regardless of training size. VGG16 &amp;amp; VGG19 Fig: A visualization of the VGG architecture. The figure presents the visualization of the VGG architecture. Images with 224 x 224 x3 dimensions are inputted to the network. Convolutions filters of only 3 x 3 are then applied with more convolutions stacked on top of each other prior to max pooling operations deeper in the architecture. The VGG network architecture was introduced by Simonyan and Zisserman in their 2014 paper, Very Deep Convolutional Networks for Large Scale Image Recognition. The VGG family of networks is characterized by using only 3 x 3 convolutional layers stacked on top of each other in increasing depth. The volume size is reduced by max pooling. Two fully-connected layers are then followed by a softmax classifier. ResNet Fig: Left: The original residual module. Right: The updated residual module using pre-activation. The ResNet module was introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition. The ResNet architecture has become a seminal work in the deep learning literature, demonstrating that extremely deep networks can be trained using standard SGD through the use of residual modules. Accuracy can be obtained by updating the residual module to use identity mappings. Inception V3 Fig: The original Inception module used in GoogLeNet. The Inception module was introduced by Szegedy et al. in their 2014 paper, Going Deeper with Convolutions. The goal of the Inception module is to act as multi-level feature extractor by computing 1 x 1, 3 x 3, and 5 x 5 convolutions within the same module of the network. The output of these filters are then stacked along the channel dimension before being fed into the next layer in the network. The original incarnation of this architecture was called GoogLeNet. Xception Xception module was introduced by Francois Chollet in their 2016 paper, Xception: Deep Learning with Depthwise Separable Convolutions. Xception is an extension to the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/Neural-Network.png" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/Neural-Network.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Neural Networks &amp;amp; CNNs</title><link href="https://thinamxx.github.io/thinam.ai/Neural%20Networks/" rel="alternate" type="text/html" title="Neural Networks &amp;amp; CNNs" /><published>2021-12-31T00:00:00-06:00</published><updated>2021-12-31T00:00:00-06:00</updated><id>https://thinamxx.github.io/thinam.ai/NeuralNetworks</id><author><name>Thinam Tamang</name></author><category term="machine learning" /><category term="natural language processing" /><category term="66DaysOfData" /><category term="notes" /><category term="neural networks" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/AI.jpg" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/AI.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>