<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://thinamxx.github.io/thinam.ai/feed.xml" rel="self" type="application/atom+xml" /><link href="https://thinamxx.github.io/thinam.ai/" rel="alternate" type="text/html" /><updated>2022-03-27T06:27:28-05:00</updated><id>https://thinamxx.github.io/thinam.ai/feed.xml</id><title type="html">THINAM TAMANG</title><subtitle>Personal website for Thinam Tamang</subtitle><entry><title type="html">Machinelearning</title><link href="https://thinamxx.github.io/thinam.ai/2022/03/27/MachineLearning.html" rel="alternate" type="text/html" title="Machinelearning" /><published>2022-03-27T00:00:00-05:00</published><updated>2022-03-27T00:00:00-05:00</updated><id>https://thinamxx.github.io/thinam.ai/2022/03/27/MachineLearning</id><author><name></name></author><summary type="html">Machine Learning</summary></entry><entry><title type="html">Pattern Recognition &amp;amp; ML</title><link href="https://thinamxx.github.io/thinam.ai/Pattern%20Recognition/" rel="alternate" type="text/html" title="Pattern Recognition &amp;amp; ML" /><published>2022-02-10T00:00:00-06:00</published><updated>2022-02-10T00:00:00-06:00</updated><id>https://thinamxx.github.io/thinam.ai/Pattern</id><author><name>Thinam Tamang</name></author><category term="machine learning" /><category term="pattern recognition" /><category term="notes" /><category term="neural networks" /><summary type="html">Introduction The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. Generalization, the ability to categorize correctly new examples that differ from those used for training is a central goal in pattern recognition. Applications in which the training data comprises examples of the input vectors with their corresponding target vectors are known as supervised learning problems. The cases such as the digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called classification. If the desired output consists of one or more continuous variables, then the task is called regression. The pattern recognition problems in which the training data consists of a set of input vectors x without any corresponding target values are called unsupervised learning problems. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization. The technique of reinforcement learning is concerned with the problem of finding suitable actions to take in a given situation in order to maximize a reward. A general feature of reinforcement learning is the trade-off between exploration, in which the system tries out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward. Probability Theory Probability theory provides a consistent framework for the quantification and manipulation of uncertainty. When combined with decision theory, it allows us to make optimal predictions given all the information available to us, even though that information may be incomplete or ambiguous. The Rules of Probability Sum Rule: p(X) = $\sum_{Y}^{}{p(X,\ Y)}$ Product Rule: p(X, Y) = p(Y / X)p(X) Here, p(X, Y) is a joint probability and is verbalized as “the probability of X and Y”. Similarly, the quantity p(Y/X) is a conditional probability and is verbalized as “the probability of Y given X”, where the quantity p(X) is a marginal probability and is verbalized as “the probability of X”. Bayes’ Theorem From the product rule, together with the symmetry property p(X, Y) = p(Y, X), we obtain the following relationship between conditional probabilities which is called Bayes’ theorem which plays a central role in pattern recognition and machine learning. Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator as being the normalization constant required ensuring that the sum of the conditional probability on the left-hand side over all values of Y equals one. p(Y / X) = $\frac{p\left( \frac{X}{Y} \right)p(X)}{p(X)}$ p(X) = $\sum_{Y}^{}{p\left( \frac{X}{Y} \right)p(Y)}$ Model Selection In the maximum likelihood approach, the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. If data is plentiful, the one approach is simply to use some of the available data to train a range of models, or a given model with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the best predictive performance. However, in many applications, the data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. If the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this problem is to use cross-validation. This allows a proportion (S – 1) / S of the available data to be used for training while making use of all of the data to assess performance. Decision Theory If we have an input vector x together with a corresponding vector t of target variables, our goal is to predict t given a new value for x. For regression problems, t will comprise continuous variables, whereas for classification problems, t will represent class labels. Minimizing Loss Function Loss function is also called a cost function, which is a single, overall measure of loss incurred in taking any of the available decisions or actions. Our goal is then to minimize the total loss incurred.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/Pattern.jpg" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/Pattern.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Convolutional Neural Networks Architectures</title><link href="https://thinamxx.github.io/thinam.ai/Convolutional%20Neural%20Networks/" rel="alternate" type="text/html" title="Convolutional Neural Networks Architectures" /><published>2022-01-22T00:00:00-06:00</published><updated>2022-01-22T00:00:00-06:00</updated><id>https://thinamxx.github.io/thinam.ai/CNNs</id><author><name>Thinam Tamang</name></author><category term="deep learning" /><category term="66DaysOfData" /><category term="notes" /><category term="cnns" /><category term="resnet" /><summary type="html">Convolutional Neural Networks The five architectures of CNNs that have been pre-trained on the ImageNet dataset and, are present in the Keras library are mentioned below: VGG16 VGG19 ResNet50 Inception V3 Xception Parameterized Learning The point of parameterized learning is to define a machine learning model that can learn patterns from our input data during training time, but have the testing process be must faster and to obtain a model that can be defined using a small number of parameters that can easily represent the network regardless of training size. VGG16 &amp;amp; VGG19 Fig: A visualization of the VGG architecture. The figure presents the visualization of the VGG architecture. Images with 224 x 224 x3 dimensions are inputted to the network. Convolutions filters of only 3 x 3 are then applied with more convolutions stacked on top of each other prior to max pooling operations deeper in the architecture. The VGG network architecture was introduced by Simonyan and Zisserman in their 2014 paper, Very Deep Convolutional Networks for Large Scale Image Recognition. The VGG family of networks is characterized by using only 3 x 3 convolutional layers stacked on top of each other in increasing depth. The volume size is reduced by max pooling. Two fully-connected layers are then followed by a softmax classifier. ResNet Fig: Left: The original residual module. Right: The updated residual module using pre-activation. The ResNet module was introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition. The ResNet architecture has become a seminal work in the deep learning literature, demonstrating that extremely deep networks can be trained using standard SGD through the use of residual modules. Accuracy can be obtained by updating the residual module to use identity mappings. Inception V3 Fig: The original Inception module used in GoogLeNet. The Inception module was introduced by Szegedy et al. in their 2014 paper, Going Deeper with Convolutions. The goal of the Inception module is to act as multi-level feature extractor by computing 1 x 1, 3 x 3, and 5 x 5 convolutions within the same module of the network. The output of these filters are then stacked along the channel dimension before being fed into the next layer in the network. The original incarnation of this architecture was called GoogLeNet. Xception Xception module was introduced by Francois Chollet in their 2016 paper, Xception: Deep Learning with Depthwise Separable Convolutions. Xception is an extension to the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/Neural-Network.png" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/Neural-Network.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Neural Networks &amp;amp; CNNs</title><link href="https://thinamxx.github.io/thinam.ai/Neural%20Networks/" rel="alternate" type="text/html" title="Neural Networks &amp;amp; CNNs" /><published>2021-12-31T00:00:00-06:00</published><updated>2021-12-31T00:00:00-06:00</updated><id>https://thinamxx.github.io/thinam.ai/NeuralNetworks</id><author><name>Thinam Tamang</name></author><category term="machine learning" /><category term="natural language processing" /><category term="66DaysOfData" /><category term="notes" /><category term="neural networks" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/AI.jpg" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/AI.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Image Classification Fundamentals</title><link href="https://thinamxx.github.io/thinam.ai/ComputerVision/" rel="alternate" type="text/html" title="Image Classification Fundamentals" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://thinamxx.github.io/thinam.ai/ImageClassification</id><author><name>Thinam Tamang</name></author><category term="machine learning" /><category term="natural language processing" /><category term="66DaysOfData" /><category term="notes" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://thinamxx.github.io/thinam.ai/images/ComputerVision.jpg" /><media:content medium="image" url="https://thinamxx.github.io/thinam.ai/images/ComputerVision.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>